{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Class Prediction of Obesity Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reding the file\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=['object']).columns: \n",
    "    print(f\"{col} : {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with 18 columns\n",
    "\n",
    "# Calculate the number of rows and columns needed for subplots\n",
    "num_rows = 6  # You can adjust this based on your preference\n",
    "num_cols = 3\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].hist(ax=axes[i], bins=20)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for i in range(len(df.columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "df_dummies = pd.get_dummies(df)\n",
    "df_dummies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df_dummies.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap (including Categorical Columns)')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_dummies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming correlation_matrix is your correlation matrix DataFrame\n",
    "\n",
    "# Create empty lists to store pairs\n",
    "positive_correlation_pairs = []\n",
    "negative_correlation_pairs = []\n",
    "\n",
    "# Iterate through the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        correlation = correlation_matrix.iloc[i, j]\n",
    "        if correlation >= 0.3:\n",
    "            positive_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation))\n",
    "        elif correlation <= -0.3:\n",
    "            negative_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation))\n",
    "\n",
    "# Convert lists to DataFrames if needed\n",
    "positive_correlation_df = pd.DataFrame(positive_correlation_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "negative_correlation_df = pd.DataFrame(negative_correlation_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df['NObeyesdad'] = ordinal_encoder.fit_transform(df[['NObeyesdad']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "df['SMOKE'] = ordinal_encoder.fit_transform(df[['SMOKE']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "df['family_history_with_overweight'] = ordinal_encoder.fit_transform(df[['family_history_with_overweight']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=['object']).columns: \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    df[col] = ordinal_encoder.fit_transform(df[[col]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's findout whethere there is noise in data or not?\n",
    "Using clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable = df['NObeyesdad']\n",
    "df = df.drop(columns=['NObeyesdad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your feature matrix\n",
    "X = df.select_dtypes(include=['number'])  # Select only numerical columns\n",
    "\n",
    "# Specify DBSCAN parameters\n",
    "epsilon = 5  # Maximum distance between points to be considered neighbors\n",
    "min_samples = 5  # Minimum number of points required to form a cluster\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "\n",
    "# Fit DBSCAN to the data\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Get cluster labels (-1 represents noise/outliers)\n",
    "cluster_labels = dbscan.labels_\n",
    "noise_indices = np.where(cluster_labels == -1)[0]\n",
    "# Plot the clustering results\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot points colored by cluster label\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=cluster_labels, cmap='viridis', s=50)\n",
    "\n",
    "# Highlight noise points\n",
    "plt.scatter(X.iloc[noise_indices, 0], X.iloc[noise_indices, 1], c='red', marker='x', label='Noise points')\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming X is your feature matrix\n",
    "X = df.select_dtypes(include=['number'])  # Select only numerical columns\n",
    "\n",
    "# Specify DBSCAN parameters\n",
    "epsilon = 5  # Maximum distance between points to be considered neighbors\n",
    "min_samples = 5  # Minimum number of points required to form a cluster\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "\n",
    "# Fit DBSCAN to the data\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Get cluster labels (-1 represents noise/outliers)\n",
    "cluster_labels = dbscan.labels_\n",
    "noise_indices = np.where(cluster_labels == -1)[0]\n",
    "\n",
    "# Assign cluster labels to the DataFrame\n",
    "df['cluster_labels'] = cluster_labels\n",
    "\n",
    "# Plot the clustering results using scatter plot matrix\n",
    "sns.pairplot(df, diag_kind='kde', hue='cluster_labels', palette='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming X is your feature matrix\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform it\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame (if needed)\n",
    "df_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# Initialize SelectKBest with the desired scoring function\n",
    "selector = SelectKBest(score_func=f_classif, k=13)  # Adjust the number of features (k) as needed\n",
    "\n",
    "# Fit the selector to your data and transform it\n",
    "X_selected = selector.fit_transform(df_normalized, lable)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FS = df_normalized[selected_features]\n",
    "X_FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# Initialize StratifiedShuffleSplit\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform stratified sampling\n",
    "for train_index, test_index in stratified_split.split(X_FS, lable):\n",
    "    X_train, X_test = X_FS.iloc[train_index], X_FS.iloc[test_index]\n",
    "    y_train, y_test = lable.iloc[train_index], lable.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier on your data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Assuming y_true are the true labels and y_pred are the predicted labels\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g', )\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holy shit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC(kernel='poly', C=1.0, gamma='scale')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not So good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Train the classifier on your data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = gb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g', )\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slightly better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier on your data\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"test.csv\")\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in testData.select_dtypes(include=['object']).columns: \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    testData[col] = ordinal_encoder.fit_transform(testData[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData= testData[X_train.columns]\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_classifier.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.read_csv(\"test.csv\")\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = id[\"id\"]\n",
    "id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = pd.DataFrame(y_pred, columns=['NObeyesdad'])\n",
    "df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming encoded_labels is your Series with encoded labels\n",
    "# Define a dictionary mapping encoded labels to original labels\n",
    "label_mapping = {0: 'Insufficient_Weight'\n",
    "                 , 1: 'Normal_Weight'\n",
    "                 , 2: 'Obesity_Type_I' \n",
    "                 , 3: 'Obesity_Type_II'\n",
    "                 , 4: 'Obesity_Type_III'\n",
    "                 , 5: 'Overweight_Level_I'\n",
    "                 , 6: 'Overweight_Level_II'}\n",
    "\n",
    "\n",
    "# Transform encoded labels back to original labels using the mapping\n",
    "original_labels = df_arr[\"NObeyesdad\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels = pd.DataFrame(original_labels, columns=['NObeyesdad'])\n",
    "original_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([id, original_labels], axis=1)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
